%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax


\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}

\usepackage[Sonny]{fncychap}
\usepackage[dontkeepoldnames]{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{图}}
\addto\captionsenglish{\renewcommand{\tablename}{表}}
\addto\captionsenglish{\renewcommand{\literalblockname}{列表}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\def\pageautorefname{page}

\setcounter{tocdepth}{1}


\usepackage[UTF8,fontset=windows]{ctex} % Chinese
\hypersetup{bookmarks,pdfstartview=FitH}


\title{Deep\_Learning\_From\_Scratch Documentation}
\date{2020 年 07 月 07 日}
\release{1.0}
\author{hyun}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{发布}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}



\chapter{感知机}
\label{\detokenize{_u611f_u77e5_u673a:deep-learning-from-scratch}}\label{\detokenize{_u611f_u77e5_u673a:id1}}\label{\detokenize{_u611f_u77e5_u673a::doc}}

\section{什么是感知机}
\label{\detokenize{_u611f_u77e5_u673a:id2}}\begin{itemize}
\item {} 
\sphinxstylestrong{感知机图示}

\end{itemize}

\noindent\sphinxincludegraphics{{perceptron}.png}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

输入
&
拥有各自权重的多个信号
\\
\hline
输出
&
一个信号（0 或 1）
\\
\hline
判断条件
&
是否超过某个阈值
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\begin{itemize}
\item {} 
\sphinxstylestrong{感知机的数学表达式}
\begin{quote}

\noindent\sphinxincludegraphics{{math1}.png}
\end{quote}

\end{itemize}


\section{感知机的实现}
\label{\detokenize{_u611f_u77e5_u673a:id3}}
使用感知机可以表示与门，或门等逻辑电路
\begin{itemize}
\item {} 
\sphinxstylestrong{与门}

\end{itemize}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{AND}\PYG{p}{(}\PYG{n}{X1}\PYG{p}{,} \PYG{n}{X2}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{w} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{b} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.7}
        \PYG{n}{tmp} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{w}\PYG{o}{*}\PYG{n}{x}\PYG{p}{)} \PYG{o}{+} \PYG{n}{b}
        \PYG{k}{if} \PYG{n}{tmp} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{:}
                \PYG{k}{return} \PYG{l+m+mi}{0}
        \PYG{k}{else}\PYG{p}{:}
                \PYG{k}{return} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxstylestrong{与非门}

\end{itemize}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{NAND}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{w} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{b} \PYG{o}{=} \PYG{l+m+mf}{0.7}
        \PYG{n}{tmp} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{w}\PYG{o}{*}\PYG{n}{x}\PYG{p}{)} \PYG{o}{+} \PYG{n}{b}
        \PYG{k}{if} \PYG{n}{tmp} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{:}
                \PYG{k}{return} \PYG{l+m+mi}{0}
        \PYG{k}{else}\PYG{p}{:}
                \PYG{k}{return} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxstylestrong{或门}

\end{itemize}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{OR}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{w} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{b} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2}
        \PYG{n}{tmp} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{w}\PYG{o}{*}\PYG{n}{x}\PYG{p}{)} \PYG{o}{+} \PYG{n}{b}
        \PYG{k}{if} \PYG{n}{tmp} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{:}
                \PYG{k}{return} \PYG{l+m+mi}{0}
        \PYG{k}{else}\PYG{p}{:}
                \PYG{k}{return} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}
\begin{description}
\item[{\sphinxstyleemphasis{重点}}] \leavevmode
\begin{DUlineblock}{0em}
\item[] w1,w2 权重是控制输入信号的重要性
\item[] b 偏置是调整神经元被激活的容易程度
\end{DUlineblock}

\end{description}


\section{多层感知机}
\label{\detokenize{_u611f_u77e5_u673a:id4}}\begin{itemize}
\item {} 
\sphinxstylestrong{感知机的局限性}
\begin{quote}

单层感知机只能表示线性空间（由直线分割的空间）。
\end{quote}

\item {} 
\sphinxstylestrong{多层感知机}
\begin{quote}

\noindent\sphinxincludegraphics{{mutiper}.png}

\begin{DUlineblock}{0em}
\item[] 研究表明激活函数使用了非线性的sigmoid函数的2层感知机可以表示任意函数。
\item[] 多层感知机理论上可以表示计算机。
\end{DUlineblock}
\end{quote}

\item {} 
\sphinxstylestrong{异或门}

\end{itemize}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{XOR}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{s1} \PYG{o}{=} \PYG{n}{NAND}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{)}
        \PYG{n}{s2} \PYG{o}{=} \PYG{n}{OR}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{)}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{AND}\PYG{p}{(}\PYG{n}{s1}\PYG{p}{,} \PYG{n}{s2}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{y}
\end{sphinxVerbatim}


\chapter{神经网络}
\label{\detokenize{_u795e_u7ecf_u7f51_u7edc:id1}}\label{\detokenize{_u795e_u7ecf_u7f51_u7edc::doc}}

\section{神经网络概述}
\label{\detokenize{_u795e_u7ecf_u7f51_u7edc:id2}}\begin{itemize}
\item {} 
\sphinxstylestrong{神经网络图示}
\begin{quote}

\noindent\sphinxincludegraphics[width=400\sphinxpxdimen]{{net}.png}
\end{quote}

神经网络有输入层（第0层），中间层（隐藏层），输出层构成。

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}

\item {} 
\sphinxstylestrong{神经网络与感知机的差异}

感知机的激活函数为阶跃函数，神经网络为其他非线性激活函数。

\end{itemize}


\section{激活函数}
\label{\detokenize{_u795e_u7ecf_u7f51_u7edc:id3}}\begin{itemize}
\item {} 
\sphinxstylestrong{阶跃函数}
\begin{quote}

\noindent\sphinxincludegraphics[width=400\sphinxpxdimen]{{step}.png}
\end{quote}

\end{itemize}
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{step\PYGZus{}function}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}
        \PYG{k}{return} \PYG{n}{y}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{恒等函数}

\end{itemize}
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{identity\PYGZus{}function}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{x}
\end{sphinxVerbatim}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{sigmoid函数}
\begin{quote}

\noindent\sphinxincludegraphics{{sigmoid_math}.png}

\noindent\sphinxincludegraphics[width=400\sphinxpxdimen]{{sigmoid}.png}
\end{quote}

\end{itemize}
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{sigmoid}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1} \PYG{o}{/} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}

\sphinxstyleemphasis{sigmoid和阶跃函数的比较}
\begin{quote}
\begin{description}
\item[{相同点}] \leavevmode
\begin{DUlineblock}{0em}
\item[] 都属于非线性激活函数
\item[] 当输入信号为重要信号时，输出较大值
\item[] 当输入信号为小信号时，输出较小的值
\item[] 输出都在0～1之间
\end{DUlineblock}

\item[{不同点}] \leavevmode
\begin{DUlineblock}{0em}
\item[] sigmoid具有平滑连续性
\end{DUlineblock}

\end{description}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{softmax函数}
\begin{quote}

\noindent\sphinxincludegraphics{{softmax_math}.png}

\noindent\sphinxincludegraphics{{softmax}.png}
\end{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{softmax}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{c} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}
        \PYG{n}{exp\PYGZus{}a} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{a} \PYG{o}{\PYGZhy{}} \PYG{n}{c}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} 溢出对策}
        \PYG{n}{sum\PYGZus{}exp\PYGZus{}a} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{exp\PYGZus{}a}\PYG{p}{)}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{exp\PYGZus{}a} \PYG{o}{/} \PYG{n}{sum\PYGZus{}exp\PYGZus{}a}

        \PYG{k}{return} \PYG{n}{y}
\end{sphinxVerbatim}

\end{itemize}
\begin{quote}

softmax函数的输出是0.0\textasciitilde{}1.0之间，并且输出值的总和为1。
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{ReLU函数}
\begin{quote}

\noindent\sphinxincludegraphics{{relu_math}.png}

\noindent\sphinxincludegraphics[width=400\sphinxpxdimen]{{relu}.png}
\end{quote}

\end{itemize}
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{relu}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}


\section{输出层设计}
\label{\detokenize{_u795e_u7ecf_u7f51_u7edc:id4}}
\begin{DUlineblock}{0em}
\item[] 神经网络可应用在分类问题和回归问题上。
\item[] 一般而言，回归问题输出层激活函数使用恒等函数，分类问题使用softmax函数。
\item[] 对于分类问题，输出层神经元的个数一般设定为类别的数量
\item[] 
\end{DUlineblock}


\section{代码实现}
\label{\detokenize{_u795e_u7ecf_u7f51_u7edc:id5}}\begin{itemize}
\item {} 
\sphinxstylestrong{三层网络示例}
\begin{quote}

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{3_net}.png}
\end{quote}

\item {} 
\sphinxstylestrong{代码示例}
\begin{enumerate}
\item {} 
初始化网络参数

\item {} 
前向传播

\end{enumerate}

\end{itemize}
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{init\PYGZus{}network}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{network} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
        \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{)}

        \PYG{k}{return} \PYG{n}{network}

\PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n}{network}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{W1}\PYG{p}{,} \PYG{n}{W2}\PYG{p}{,} \PYG{n}{W3} \PYG{o}{=} \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
        \PYG{n}{b1}\PYG{p}{,} \PYG{n}{b2}\PYG{p}{,} \PYG{n}{b3} \PYG{o}{=} \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{network}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

        \PYG{n}{a1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{W1}\PYG{p}{)} \PYG{o}{+} \PYG{n}{b1}
        \PYG{n}{z1} \PYG{o}{=} \PYG{n}{sigmoid}\PYG{p}{(}\PYG{n}{a1}\PYG{p}{)}
        \PYG{n}{a2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{a1}\PYG{p}{,} \PYG{n}{W2}\PYG{p}{)} \PYG{o}{+} \PYG{n}{b2}
        \PYG{n}{z2} \PYG{o}{=} \PYG{n}{sigmoid}\PYG{p}{(}\PYG{n}{a2}\PYG{p}{)}
        \PYG{n}{a3} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{a2}\PYG{p}{,} \PYG{n}{W3}\PYG{p}{)} \PYG{o}{+} \PYG{n}{b3}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{identity\PYGZus{}function}\PYG{p}{(}\PYG{n}{a3}\PYG{p}{)}

        \PYG{k}{return} \PYG{n}{y}

\PYG{n}{network} \PYG{o}{=} \PYG{n}{init\PYGZus{}network}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{forward}\PYG{p}{(}\PYG{n}{network}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}


\chapter{神经网络的学习}
\label{\detokenize{_u795e_u7ecf_u7f51_u7edc_u7684_u5b66_u4e60:id1}}\label{\detokenize{_u795e_u7ecf_u7f51_u7edc_u7684_u5b66_u4e60::doc}}

\section{从数据中学习}
\label{\detokenize{_u795e_u7ecf_u7f51_u7edc_u7684_u5b66_u4e60:id2}}
\sphinxstylestrong{神经网络的特征就是可以自动从数据中学习。}
\begin{itemize}
\item {} 
训练集和测试集

\end{itemize}
\begin{quote}

\begin{DUlineblock}{0em}
\item[] 使用训练集来进行学习，寻找最优参数
\item[] 使用测试集来评估模型的泛化能力（泛化能力是机器学习的最终目标）。
\end{DUlineblock}
\end{quote}


\section{损失函数}
\label{\detokenize{_u795e_u7ecf_u7f51_u7edc_u7684_u5b66_u4e60:id3}}
\sphinxstylestrong{损失函数是用来评价神经网络学习性能的指标。}
\begin{itemize}
\item {} \begin{description}
\item[{\sphinxstylestrong{均方误差}}] \leavevmode
\noindent\sphinxincludegraphics{{msn}.png}

\end{description}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{mean\PYGZus{}squares\PYGZus{}error}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mf}{0.5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y} \PYG{o}{\PYGZhy{}} \PYG{n}{t}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\item {} \begin{description}
\item[{\sphinxstylestrong{交叉熵误差}}] \leavevmode
\noindent\sphinxincludegraphics{{cross}.png}

\end{description}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{cross\PYGZus{}entropy\PYGZus{}error}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{delta} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}7}
        \PYG{k}{return} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{t} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{y} \PYG{o}{+} \PYG{n}{delta}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\item {} 
\sphinxstylestrong{mini-batch交叉熵误差}
\begin{quote}

\noindent\sphinxincludegraphics{{batch_cross}.png}
\end{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{cross\PYGZus{}entropy\PYGZus{}error}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{y}\PYG{o}{.}\PYG{n}{ndim} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{:}
                \PYG{n}{t} \PYG{o}{=} \PYG{n}{t}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{t}\PYG{o}{.}\PYG{n}{size}\PYG{p}{)}
                \PYG{n}{y} \PYG{o}{=} \PYG{n}{t}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{size}\PYG{p}{)}

        \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{k}{return} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{t} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{y} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}7}\PYG{p}{)}\PYG{p}{)} \PYG{o}{/} \PYG{n}{batch\PYGZus{}size}
\end{sphinxVerbatim}

\end{itemize}


\section{数值微分和梯度}
\label{\detokenize{_u795e_u7ecf_u7f51_u7edc_u7684_u5b66_u4e60:id4}}\begin{itemize}
\item {} 
\sphinxstylestrong{导数}
\begin{quote}

\noindent\sphinxincludegraphics{{diff}.png}
\end{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{numerical\PYGZus{}diff}\PYG{p}{(}\PYG{n}{f}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{h} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}4} \PYG{c+c1}{\PYGZsh{} 0.0001}
        \PYG{k}{return} \PYG{p}{(}\PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{o}{+}\PYG{n}{h}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{)}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{h}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} 中心差分}
\end{sphinxVerbatim}

\item {} 
\sphinxstylestrong{偏导数}
\begin{quote}

\noindent\sphinxincludegraphics{{bias_diff}.png}
\end{quote}

\item {} 
\sphinxstylestrong{梯度}
\begin{quote}

\noindent\sphinxincludegraphics{{gradient}.png}
\end{quote}

\begin{DUlineblock}{0em}
\item[] 由全部变量的偏导数汇总而成的向量称为梯度。
\item[] 梯度表示的是各点处的函数值减小最多的方向。
\item[] 函数的极小值，最小值及鞍点的地方，梯度为0。虽然梯度方向不一定指向最小值，但沿着它的方向能最大限度地减小函数的值。
\item[] 
\end{DUlineblock}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{numerical\PYGZus{}gradient}\PYG{p}{(}\PYG{n}{f}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{h} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}4} \PYG{c+c1}{\PYGZsh{} 0.0001}
        \PYG{n}{grad} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}like}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} 生成和x形状相同的数组}

        \PYG{k}{for} \PYG{n}{idx} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{tmp\PYGZus{}val} \PYG{o}{=} \PYG{n}{x}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]}
                \PYG{c+c1}{\PYGZsh{} f(x+h)的计算}
                \PYG{n}{x}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]} \PYG{o}{=} \PYG{n}{tmp\PYGZus{}val} \PYG{o}{+} \PYG{n}{h}
                \PYG{n}{fxh1} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

                \PYG{c+c1}{\PYGZsh{} f(x\PYGZhy{}h)的计算}
                \PYG{n}{x}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]} \PYG{o}{=} \PYG{n}{tmp\PYGZus{}val} \PYG{o}{\PYGZhy{}}\PYG{n}{h}
                \PYG{n}{fxh2} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

                \PYG{n}{grad}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]} \PYG{o}{=} \PYG{p}{(}\PYG{n}{fxh1} \PYG{o}{\PYGZhy{}} \PYG{n}{fxh2}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{h}\PYG{p}{)}
                \PYG{n}{x}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]} \PYG{o}{=} \PYG{n}{tmp\PYGZus{}val} \PYG{c+c1}{\PYGZsh{} 还原值}

        \PYG{k}{return} \PYG{n}{grad}
\end{sphinxVerbatim}

\item {} 
\sphinxstylestrong{梯度法}
\begin{quote}

\noindent\sphinxincludegraphics[width=150\sphinxpxdimen]{{gradient_method}.png}
\end{quote}

\begin{DUlineblock}{0em}
\item[] \(\eta\)称为学习率
\end{DUlineblock}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{gradient\PYGZus{}descent}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{init\PYGZus{}x}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{step\PYGZus{}num}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{init\PYGZus{}x}

        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{step\PYGZus{}num}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{grad} \PYG{o}{=} \PYG{n}{numerical\PYGZus{}gradient}\PYG{p}{(}\PYG{n}{f}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}
                \PYG{n}{x} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n}{lr} \PYG{o}{*} \PYG{n}{grad}

        \PYG{k}{return} \PYG{n}{x}
\end{sphinxVerbatim}

\end{itemize}


\section{学习算法步骤}
\label{\detokenize{_u795e_u7ecf_u7f51_u7edc_u7684_u5b66_u4e60:id5}}\begin{enumerate}
\item {} 
\sphinxstylestrong{mini-batch}  从训练数据中随机选取一部分数据，目标是减小mini-batch的值。

\item {} 
\sphinxstylestrong{计算梯度} 为减小nimi-batch损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向。

\item {} 
\sphinxstylestrong{更新参数} 将权重参数沿梯度方向进行微小更新。

\item {} 
\sphinxstylestrong{重复} 重复步骤1，2，3。

\end{enumerate}


\chapter{误差反向传播法}
\label{\detokenize{_u8bef_u5dee_u53cd_u5411_u4f20_u64ad_u6cd5:id1}}\label{\detokenize{_u8bef_u5dee_u53cd_u5411_u4f20_u64ad_u6cd5::doc}}

\section{计算图}
\label{\detokenize{_u8bef_u5dee_u53cd_u5411_u4f20_u64ad_u6cd5:id2}}\begin{itemize}
\item {} 
\sphinxstylestrong{用计算图求解}
\begin{quote}

\noindent\sphinxincludegraphics{{cal}.png}
\end{quote}

\item {} 
\sphinxstylestrong{为何要使用计算图}
\begin{enumerate}
\item {} 
局部计算，简化问题

\item {} 
可以将中间结果保存起来

\item {} 
可以通过反向传播高效计算导数

\end{enumerate}

\end{itemize}


\section{链式法则}
\label{\detokenize{_u8bef_u5dee_u53cd_u5411_u4f20_u64ad_u6cd5:id3}}
如果某个函数由符合函数表示，则该符合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。
\begin{quote}

\noindent\sphinxincludegraphics{{de}.png}
\end{quote}
\begin{itemize}
\item {} \begin{description}
\item[{\sphinxstylestrong{链式法则和计算图}}] \leavevmode
\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{4_3}.png}

\end{description}

\end{itemize}


\section{反向传播}
\label{\detokenize{_u8bef_u5dee_u53cd_u5411_u4f20_u64ad_u6cd5:id4}}\begin{itemize}
\item {} 
\sphinxstylestrong{加法节点的反向传播}
\begin{quote}

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{4_4}.png}
\end{quote}

\begin{DUlineblock}{0em}
\item[] 加法节点的反向传播将上游的值原封不动地输出到下游
\item[] 
\end{DUlineblock}

\item {} 
\sphinxstylestrong{乘法节点的反向传播}
\begin{quote}

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{4_5}.png}
\end{quote}

\begin{DUlineblock}{0em}
\item[] 乘法的反向传播会乘以输入信号的翻转值
\item[] 
\end{DUlineblock}

\end{itemize}


\section{层的实现}
\label{\detokenize{_u8bef_u5dee_u53cd_u5411_u4f20_u64ad_u6cd5:id5}}
层的实现中有两个共通的方法forward()和backword()。

\sphinxstylestrong{简单层的实现}
\begin{itemize}
\item {} 
乘法层
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{MulLayer}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
                \PYG{c+c1}{\PYGZsh{} 初始化实例变量x和y，用于保存正向传播时的输入值}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x} \PYG{o}{=} \PYG{n+nb+bp}{None}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y} \PYG{o}{=} \PYG{n+nb+bp}{None}

        \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x} \PYG{o}{=} \PYG{n}{x}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y} \PYG{o}{=} \PYG{n}{y}
                \PYG{n}{out} \PYG{o}{=} \PYG{n}{x} \PYG{o}{*} \PYG{n}{y}

                \PYG{k}{return} \PYG{n}{out}

        \PYG{k}{def} \PYG{n+nf}{backword}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dout}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{dx} \PYG{o}{=} \PYG{n}{dout} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y} \PYG{c+c1}{\PYGZsh{} 翻转x和y}
                \PYG{n}{dy} \PYG{o}{=} \PYG{n}{dout} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x}

                \PYG{k}{return} \PYG{n}{dx}\PYG{p}{,} \PYG{n}{dy}
\end{sphinxVerbatim}
\end{quote}

\item {} 
加法层
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{AddLayer}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
                \PYG{k}{pass}

        \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{out} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{n}{y}
                \PYG{k}{return} \PYG{n}{out}

        \PYG{k}{def} \PYG{n+nf}{backward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dout}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{dx} \PYG{o}{=} \PYG{n}{dout} \PYG{o}{*} \PYG{l+m+mi}{1}
                \PYG{n}{dy} \PYG{o}{=} \PYG{n}{dout} \PYG{o}{*} \PYG{l+m+mi}{1}
                \PYG{k}{return} \PYG{n}{dx}\PYG{p}{,} \PYG{n}{dy}
\end{sphinxVerbatim}
\end{quote}

\end{itemize}

\sphinxstylestrong{激活函数层的实现}

在神经网络层的实现中，一般假定forward()和backward()的参数是NumPy数组。
\begin{itemize}
\item {} 
ReLU层
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{Relu}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mask} \PYG{o}{=} \PYG{n+nb+bp}{None}

        \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mask} \PYG{o}{=} \PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZlt{}}\PYG{o}{=}\PYG{l+m+mi}{0} \PYG{p}{)}
                \PYG{n}{out} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
                \PYG{n}{out}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mask}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}

                \PYG{k}{return} \PYG{n}{out}

        \PYG{k}{def} \PYG{n+nf}{backward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dout}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{dout}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mask}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}
                \PYG{n}{dx} \PYG{o}{=} \PYG{n}{dout}

                \PYG{k}{return} \PYG{n}{dx}
\end{sphinxVerbatim}
\end{quote}

\item {} 
Sigmoid层
\begin{quote}

\noindent\sphinxincludegraphics{{4_6}.png}

\noindent\sphinxincludegraphics{{4_7}.png}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{Sigmoid}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{out} \PYG{o}{=} \PYG{n+nb+bp}{None}

        \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{out} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{/} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{out} \PYG{o}{=} \PYG{n}{out}

                \PYG{k}{return} \PYG{n}{out}

        \PYG{k}{def} \PYG{n+nf}{backward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dout}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{dx} \PYG{o}{=} \PYG{n}{dout} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{out}\PYG{p}{)} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{out}

                \PYG{k}{return} \PYG{n}{dx}
\end{sphinxVerbatim}
\end{quote}

\item {} 
\sphinxstylestrong{Affine层的实现}

神经网络的正向传播中进行的矩阵乘积运算在几何学领域被成为“仿射变换”
\begin{quote}

\noindent\sphinxincludegraphics{{affine}.png}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{Affine}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{W}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{W} \PYG{o}{=} \PYG{n}{W}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{b} \PYG{o}{=} \PYG{n}{b}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x} \PYG{o}{=} \PYG{n+nb+bp}{None}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dW} \PYG{o}{=} \PYG{n+nb+bp}{None}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{db} \PYG{o}{=} \PYG{n+nb+bp}{None}

        \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x} \PYG{o}{=} \PYG{n}{x}
                \PYG{n}{out} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{W}\PYG{p}{)} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{b}

                \PYG{k}{return} \PYG{n}{out}

        \PYG{k}{def} \PYG{n+nf}{backword}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dout}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{dx} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{dout}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{W}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dW} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{dout}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{db} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{dout}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

                \PYG{k}{return} \PYG{n}{dx}
\end{sphinxVerbatim}
\end{quote}

\item {} 
\sphinxstylestrong{Softmax-with-Loss}

神经网络中进行的处理有推理和学习两个阶段。推理通常不使用softmax层，学习阶段需要softmax层。
\begin{quote}

\noindent\sphinxincludegraphics{{4_9}.png}

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{4_8}.png}
\end{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{SoftmaxWithLoss}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss} \PYG{o}{=} \PYG{n+nb+bp}{None} \PYG{c+c1}{\PYGZsh{} 损失}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y} \PYG{o}{=} \PYG{n+nb+bp}{None}    \PYG{c+c1}{\PYGZsh{} softmax 输出}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{t} \PYG{o}{=} \PYG{n+nb+bp}{None}    \PYG{c+c1}{\PYGZsh{} 监督数据（one\PYGZhy{}hot vector）}

        \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{t} \PYG{o}{=} \PYG{n}{t}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y} \PYG{o}{=} \PYG{n}{softmax}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss} \PYG{o}{=} \PYG{n}{cross\PYGZus{}entropy\PYGZus{}error}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{t}\PYG{p}{)}

                \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss}

        \PYG{k}{def} \PYG{n+nf}{backward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dout}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{t}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
                \PYG{n}{dx} \PYG{o}{=} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{t}\PYG{p}{)} \PYG{o}{/} \PYG{n}{batch\PYGZus{}size}

                \PYG{k}{return} \PYG{n}{dx}
\end{sphinxVerbatim}

\end{itemize}


\section{误差反向传播法的实现}
\label{\detokenize{_u8bef_u5dee_u53cd_u5411_u4f20_u64ad_u6cd5:id6}}\begin{itemize}
\item {} 
\sphinxstylestrong{误差反向传播法的神经网络}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{TwoLayerNet}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{input\PYGZus{}size}\PYG{p}{,} \PYG{n}{hidden\PYGZus{}size}\PYG{p}{,} \PYG{n}{output\PYGZus{}size}\PYG{p}{,}
                                \PYG{n}{weight\PYGZus{}init\PYGZus{}std}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{:}
                \PYG{c+c1}{\PYGZsh{} 初始化权重}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{param} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{weight\PYGZus{}init\PYGZus{}std} \PYG{o}{*} \PYGZbs{}
                                \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{input\PYGZus{}size}\PYG{p}{,} \PYG{n}{hidden\PYGZus{}size}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}size}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{weight\PYGZus{}init\PYGZus{}std} \PYG{o}{*} \PYGZbs{}
                                \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}size}\PYG{p}{,} \PYG{n}{output\PYGZus{}size}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{output\PYGZus{}size}\PYG{p}{)}

                \PYG{c+c1}{\PYGZsh{} 生成层}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers} \PYG{o}{=} \PYG{n}{OrderedDict}\PYG{p}{(}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYGZbs{}
                        \PYG{n}{Affine}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{Relu}\PYG{p}{(}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYGZbs{}
                        \PYG{n}{Affine}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lastLayer} \PYG{o}{=} \PYG{n}{SoftmaxWithLoss}\PYG{p}{(}\PYG{p}{)}

        \PYG{k}{def} \PYG{n+nf}{predict}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
                \PYG{k}{for} \PYG{n}{layer} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                        \PYG{n}{x} \PYG{o}{=} \PYG{n}{layer}\PYG{o}{.}\PYG{n}{forward}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

                \PYG{k}{return} \PYG{n}{x}

        \PYG{k}{def} \PYG{n+nf}{loss}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{y} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
                \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Lastlayer}\PYG{o}{.}\PYG{n}{forward}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}

        \PYG{k}{def} \PYG{n+nf}{accuracy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{y} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
                \PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
                \PYG{k}{if} \PYG{n}{t}\PYG{o}{.}\PYG{n}{ndim} \PYG{o}{!=} \PYG{l+m+mi}{1}\PYG{p}{:}
                        \PYG{n}{t} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
                        \PYG{n}{accuracy} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{y} \PYG{o}{==} \PYG{n}{t}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{float}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
                \PYG{k}{return} \PYG{n}{accuracy}

        \PYG{c+c1}{\PYGZsh{} x:输入数据，t:监督数据}
        \PYG{k}{def} \PYG{n+nf}{gradient}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}\PYG{p}{:}
                \PYG{c+c1}{\PYGZsh{} forward}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}

                \PYG{c+c1}{\PYGZsh{} backward}
                \PYG{n}{dout} \PYG{o}{=} \PYG{l+m+mi}{1}
                \PYG{n}{dout} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Lastlayer}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{n}{dout}\PYG{p}{)}

                \PYG{n}{layers} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
                \PYG{n}{layers}\PYG{o}{.}\PYG{n}{reverse}\PYG{p}{(}\PYG{p}{)}
                \PYG{k}{for} \PYG{n}{layer} \PYG{o+ow}{in} \PYG{n}{layers}\PYG{p}{:}
                        \PYG{n}{dout} \PYG{o}{=} \PYG{n}{layer}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{n}{dout}\PYG{p}{)}

                \PYG{c+c1}{\PYGZsh{} 设定}
                \PYG{n}{grads} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
                \PYG{n}{grads}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dW}
                \PYG{n}{grads}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{db}
                \PYG{n}{grads}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dW}
                \PYG{n}{grads}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{db}

                \PYG{k}{return} \PYG{n}{grads}
\end{sphinxVerbatim}

\item {} 
\sphinxstylestrong{误差反向传播法的学习}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} 读入数据}
\PYG{p}{(}\PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{t\PYGZus{}train}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{n}{x\PYGZus{}test}\PYG{p}{,} \PYG{n}{t\PYGZus{}test}\PYG{p}{)} \PYG{o}{=} \PYGZbs{}
        \PYG{n}{load\PYGZus{}mnist}\PYG{p}{(}\PYG{n}{normalize}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,} \PYG{n}{one\PYGZus{}hot\PYGZus{}label}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}

\PYG{n}{network} \PYG{o}{=} \PYG{n}{TwoLayerNet}\PYG{p}{(}\PYG{n}{input\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{784}\PYG{p}{,} \PYG{n}{hidden\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{output\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{n}{iters\PYGZus{}num} \PYG{o}{=} \PYG{l+m+mi}{10000}
\PYG{n}{train\PYGZus{}size} \PYG{o}{=} \PYG{n}{x\PYGZus{}train}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{leraning\PYGZus{}rate} \PYG{o}{=} \PYG{l+m+mf}{0.01}
\PYG{n}{train\PYGZus{}loss\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{n}{train\PYGZus{}acc\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{n}{test\PYGZus{}acc\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

\PYG{n}{iter\PYGZus{}per\PYGZus{}epoch} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{n}{train\PYGZus{}size} \PYG{o}{/} \PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{iters\PYGZus{}num}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{batch\PYGZus{}mask} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{train\PYGZus{}size}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{p}{)}
        \PYG{n}{x\PYGZus{}batch} \PYG{o}{=} \PYG{n}{x\PYGZus{}train}\PYG{p}{(}\PYG{n}{batch\PYGZus{}mask}\PYG{p}{)}
        \PYG{n}{t\PYGZus{}batch} \PYG{o}{=} \PYG{n}{t\PYGZus{}train}\PYG{p}{(}\PYG{n}{batch\PYGZus{}mask}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} 通过误差反向传播法求梯度}
        \PYG{n}{grad} \PYG{o}{=} \PYG{n}{network}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{x\PYGZus{}batch}\PYG{p}{,} \PYG{n}{t\PYGZus{}batch}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} 更新}
        \PYG{k}{for} \PYG{n}{key} \PYG{o+ow}{in} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{network}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n}{learning\PYGZus{}rate} \PYG{o}{*} \PYG{n}{grad}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]}

        \PYG{n}{loss} \PYG{o}{=} \PYG{n}{network}\PYG{o}{.}\PYG{n}{loss}\PYG{p}{(}\PYG{n}{x\PYGZus{}batch}\PYG{p}{,} \PYG{n}{t\PYGZus{}batch}\PYG{p}{)}
        \PYG{n}{train\PYGZus{}loss\PYGZus{}list}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{loss}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{i} \PYG{o}{\PYGZpc{}} \PYG{n}{iter\PYGZus{}per\PYGZus{}epoch} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
                \PYG{n}{train\PYGZus{}acc} \PYG{o}{=} \PYG{n}{network}\PYG{o}{.}\PYG{n}{accuracy}\PYG{p}{(}\PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{t\PYGZus{}trian}\PYG{p}{)}
                \PYG{n}{test\PYGZus{}acc} \PYG{o}{=} \PYG{n}{network}\PYG{o}{.}\PYG{n}{accuracy}\PYG{p}{(}\PYG{n}{x\PYGZus{}test}\PYG{p}{,} \PYG{n}{t\PYGZus{}test}\PYG{p}{)}
                \PYG{n}{train\PYGZus{}acc\PYGZus{}list}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{train\PYGZus{}acc}\PYG{p}{)}
                \PYG{n}{test\PYGZus{}acc\PYGZus{}list}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{test\PYGZus{}acc}\PYG{p}{)}
                \PYG{k}{print}\PYG{p}{(}\PYG{n}{train\PYGZus{}acc}\PYG{p}{,} \PYG{n}{test\PYGZus{}acc}\PYG{p}{)}
\end{sphinxVerbatim}

\end{itemize}


\chapter{与学习相关的技巧}
\label{\detokenize{_u4e0e_u5b66_u4e60_u76f8_u5173_u7684_u6280_u5de7:id1}}\label{\detokenize{_u4e0e_u5b66_u4e60_u76f8_u5173_u7684_u6280_u5de7::doc}}

\section{参数的更新}
\label{\detokenize{_u4e0e_u5b66_u4e60_u76f8_u5173_u7684_u6280_u5de7:id2}}
神经网络学习的目的是找到使损失函数的值尽可能小的参数。
\begin{itemize}
\item {} 
\sphinxstylestrong{SGD}
\begin{equation*}
\begin{split}\pmb{W} \leftarrow \pmb{W} - \eta\frac{\partial{L}}{\partial\pmb{W}}\end{split}
\end{equation*}
\end{itemize}
\begin{quote}

将SGD实现为一个Python的类
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{SGD}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{lr}

        \PYG{k}{def} \PYG{n+nf}{update}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{params}\PYG{p}{,} \PYG{n}{grads}\PYG{p}{)}\PYG{p}{:}
                \PYG{k}{for} \PYG{n}{key} \PYG{o+ow}{in} \PYG{n}{params}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                        \PYG{n}{params}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{*} \PYG{n}{grads}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]}
\end{sphinxVerbatim}
\end{quote}

使用SGD，可以按如下方式进行神经网络的参数更新
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{network} \PYG{o}{=} \PYG{n}{TwoLayerNet}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{SGD}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10000}\PYG{p}{)}\PYG{p}{:}
        \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
        \PYG{n}{x\PYGZus{}batch}\PYG{p}{,} \PYG{n}{t\PYGZus{}batch} \PYG{o}{=} \PYG{n}{get\PYGZus{}mini\PYGZus{}batch}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} mini\PYGZus{}batch}
        \PYG{n}{grads} \PYG{o}{=} \PYG{n}{network}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{x\PYGZus{}batch}\PYG{p}{,} \PYG{n}{t\PYGZus{}batch}\PYG{p}{)}
        \PYG{n}{params} \PYG{o}{=} \PYG{n}{network}\PYG{o}{.}\PYG{n}{params}
        \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{n}{params}\PYG{p}{,} \PYG{n}{grads}\PYG{p}{)}
        \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}
\end{quote}

\sphinxstyleemphasis{SGD的缺点}

\begin{DUlineblock}{0em}
\item[] 如果函数的形状非均向，搜索的路径就会非常低效
\item[] SGD低效的根本原因是梯度并没有指向最小值的方向
\end{DUlineblock}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{Momentum}

\end{itemize}
\begin{quote}

Momentum是在指数加权平均的基础上提出的

\sphinxstylestrong{指数加权平均可用如下公式表示：}
\begin{quote}
\begin{equation*}
\begin{split}v_t=k * v_{t-1} + (1-k)w_t\end{split}
\end{equation*}\end{quote}

\begin{DUlineblock}{0em}
\item[] 核心：使用前多少个值来表示当前的趋势值Vt
\item[] k的值决定了趋势值v应受到前多少个值的影响，w代表当前值
\item[] 参考文档 \sphinxurl{http://captainbed.top/2-2-3/}
\item[] 用一行代码表示指数加权平均：
\end{DUlineblock}
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{v} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{l+m+mi}{100}
\PYG{n}{v} \PYG{o}{=} \PYG{n}{kv} \PYG{o}{+} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{k}\PYG{p}{)}\PYG{n}{w\PYGZus{}t}
\end{sphinxVerbatim}
\end{quote}

Momentum的公式表示如下：
\begin{quote}
\begin{equation*}
\begin{split}v \leftarrow \alpha{v} - \eta\frac{\partial{L}}{\partial\pmb{W}}\\
\pmb{W} \leftarrow \pmb{w} + v\end{split}
\end{equation*}\end{quote}

将Momentum实现为一个Python类
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{Momentum}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{momentum}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{lr}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{momentum} \PYG{o}{=} \PYG{n}{momentum}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{v} \PYG{o}{=} \PYG{n+nb+bp}{None}

        \PYG{k}{def} \PYG{n+nf}{update}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{params}\PYG{p}{,} \PYG{n}{grads}\PYG{p}{)}\PYG{p}{:}
                \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{v} \PYG{o+ow}{is} \PYG{n+nb+bp}{None}\PYG{p}{:}
                        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{v} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
                        \PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{val} \PYG{o+ow}{in} \PYG{n}{params}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{v}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}like}\PYG{p}{(}\PYG{n}{val}\PYG{p}{)}

                \PYG{k}{for} \PYG{n}{key} \PYG{o+ow}{in} \PYG{n}{params}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{v}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{momentum}\PYG{o}{*}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{v}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr}\PYG{o}{*}\PYG{n}{grads}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]}
                        \PYG{n}{params}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{v}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]}
\end{sphinxVerbatim}
\end{quote}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{AdaGrad}

\end{itemize}
\begin{quote}

AdaGrad会为参数的每个元素适当地调整学习率

AdaGrad的公式如下：
\begin{quote}
\begin{equation*}
\begin{split}\pmb{h} \leftarrow \pmb{h} + \frac{\partial{L}}{\partial\pmb{W}} \odot \frac{\partial{L}}{\partial\pmb{W}}\\
\pmb{W} \leftarrow \pmb{W} - \eta\frac{1}{\sqrt{\pmb{h}}}\frac{\partial{L}}{\partial\pmb{W}}\end{split}
\end{equation*}\end{quote}

这里出现了新变量h，它保存了以前的所有梯度的平方和，在更新参数时，通过乘以其根号倒数，就可以调整学习的尺度。这意味着参数中的元素中变动较大（被大幅更新的元素的学习率会将变小）。

将AdaGrad实现为一个Python类
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{AdaGrad}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{lr}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{h} \PYG{o}{=} \PYG{n+nb+bp}{None}

        \PYG{k}{def} \PYG{n+nf}{update}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{params}\PYG{p}{,} \PYG{n}{grads}\PYG{p}{)}\PYG{p}{:}
                \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{h} \PYG{o+ow}{is} \PYG{n+nb+bp}{None}\PYG{p}{:}
                        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{h} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
                \PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{val} \PYG{o+ow}{in} \PYG{n}{params}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{h}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}like}\PYG{p}{(}\PYG{n}{val}\PYG{p}{)}

        \PYG{k}{for} \PYG{n}{key} \PYG{o+ow}{in} \PYG{n}{params}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{h}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{grads}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{*} \PYG{n}{grads}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]}
                \PYG{n}{parsms}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{*} \PYG{n}{grads}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{/} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{h}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}7}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{RMSprop}

\end{itemize}
\begin{quote}

AdaGrad会记录过去所有的梯度平方和，而RMSprop则逐渐遗忘过去的梯度，在做加法时将新梯度的信息更多的反映出来。
\begin{enumerate}
\item {} 
首先计算出dw和db

\item {} 
计算出指数平均

\end{enumerate}
\begin{quote}
\begin{equation*}
\begin{split}s_{dw} = ks_{dw} + (1-k)dw^{2}\\
s_{db} = ks_{db} + (1-k)db^{2}\end{split}
\end{equation*}\end{quote}
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
更新w和b

\end{enumerate}
\begin{quote}
\begin{equation*}
\begin{split}w = w - r(dw/sqrt(s_{dw}))\\
b = b - r(db/sqrt(s_{db}))\end{split}
\end{equation*}\end{quote}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{Adam}

\end{itemize}
\begin{quote}

Adam融合了Momentum和AdaGrad方法
\begin{enumerate}
\item {} 
算出dw，db

\item {} 
求出动量指数平均

\end{enumerate}
\begin{quote}
\begin{equation*}
\begin{split}v_{dw} = k_1v_{dw}+(1-k_1)dw\\
v_{db} = k_1v_{db}+(1-k_1)db\end{split}
\end{equation*}\end{quote}
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
求出RMPprop指数平均

\end{enumerate}
\begin{quote}
\begin{equation*}
\begin{split}s_{dw} = k_2s_{dw}+(1-k_2)dw^{2}\\
s_{db} = k_2s_{db}+(1-k_2)db^{2}\end{split}
\end{equation*}\end{quote}
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
对指数平均进行修正

\end{enumerate}
\begin{quote}
\begin{equation*}
\begin{split}v^{c}dw = v_{dw}/(1-k_1^{t})\\
v^{c}db = v_{db}/(1-k_1^{t})\\
s^{c}dw = s_{dw}/(1-k_2^{t})\\
    s^{c}db = s_{db}/(1-k_2^{t})\end{split}
\end{equation*}\end{quote}
\begin{enumerate}
\setcounter{enumi}{4}
\item {} 
通过两个指数平均更新参数

\end{enumerate}
\begin{quote}
\begin{equation*}
\begin{split}w = w - r(v^{c}_{dw}/sqrt(s^{c}_{dw}+u))\\
b = b - r(v^{c}_{db}/sqrt(s^{c}_{db}+u))\end{split}
\end{equation*}\end{quote}
\end{quote}


\section{权重的初始值}
\label{\detokenize{_u4e0e_u5b66_u4e60_u76f8_u5173_u7684_u6280_u5de7:id3}}\begin{itemize}
\item {} 
\sphinxstylestrong{权重初始值能设置成0吗？}

\end{itemize}
\begin{quote}

将权重初始值设置成一样的值，在误差反向传播法中，所有的权重值会进行相同的更新，拥有了对称的值（重复的值）。为防止“权重均一化”，必须随机生成初始值。
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{隐藏层激活值的分布}

\end{itemize}
\begin{quote}

各层激活值的分布要求要适当的广度，通过在各层之间传递多样性的数据，神经网络就可以进行高效的学习。如果传递的是有所偏向的数据，就会出现梯度消失或“表现力受限”的问题，导致学习可能无法顺利进行。
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{Xavier初始值}

\end{itemize}
\begin{quote}

如果前一层的节点数为
\(n\)
，则初始值使用标准差为
\(\frac{1}{\sqrt{n}}\)
的分布

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{node\PYGZus{}num} \PYG{o}{=} \PYG{l+m+mi}{100} \PYG{c+c1}{\PYGZsh{} 前一层的节点数}
\PYG{n}{w} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{node\PYGZus{}num}\PYG{p}{,} \PYG{n}{node\PYGZus{}num}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{node\PYGZus{}num}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{ReLU的权重初始值He}

\end{itemize}
\begin{quote}

当前一层的节点数为
\(n\)
，则初始值使用标准差为
\(\sqrt{\frac{2}{n}}\)
的分布

激活函数使用ReLU时，不同权重初始值的激活值分布的变化如下图

\noindent{\hspace*{\fill}\sphinxincludegraphics{{activations}.png}\hspace*{\fill}}

\sphinxstylestrong{当激活函数使用ReLU时，权重初始值使用He初始值。当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值。}
\end{quote}


\section{Batch Normalization}
\label{\detokenize{_u4e0e_u5b66_u4e60_u76f8_u5173_u7684_u6280_u5de7:batch-normalization}}\begin{quote}

Batch Norm的思路是调整各层激活值分布使其拥有适当的广度。为此，要向神经网络中插入对数据分布进行正规化的层，即Batch Normalization曾。

\noindent{\hspace*{\fill}\sphinxincludegraphics{{batch}.png}\hspace*{\fill}}

Batch Norm以进行学习时的nimi-batch为单位，按mini-batch进行正规化。具体而言就是使数据分布的均值为0，方差为1的正规化。如下式表示：
\begin{equation*}
\begin{split}\begin{align}
    \mu_B \leftarrow \frac{1}{m}\sum_{i=1}^{m}x_i\\
\sigma^{2}_B \leftarrow \frac{1}{m}\sum_{i=1}^m{(x_i - \mu_B)}^{2}\\
    \hat{x}_i \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma^{2}_B + \epsilon}}
    \end{align}\end{split}
\end{equation*}
nimi-batch将输入的数据
\(\{x_1,x_2,...,x_m\}\)
变换为均值为0，方差为1的数据
\(\{\hat{x}_1,\hat{x}_2,...,\hat{x}_m\}\)
。通过将这个处理插入刀激活函数的前面（或者后面），可以减小数据分布的偏向。接着，Batch Norm会对正规化后的数据进行缩放和平移的变换：
\begin{equation*}
\begin{split}y_i \leftarrow \gamma\hat{x}_i + \beta\end{split}
\end{equation*}
这里，
\(\gamma\)
和
\(\beta\)
是参数。一开始
\(\gamma = 1, \beta = 0\)
，然后再通过学习调整到合适的值。
\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Batch Norm的优点：}
\item[] 1. 可以使学习快速进行（可以增大学习率）
\item[] 2. 不那么依赖初始值（对于初始值不那么神经质）
\item[] 3. 抑制过拟合（降低Dropout等的必要性）
\item[] 
\end{DUlineblock}


\section{正则化}
\label{\detokenize{_u4e0e_u5b66_u4e60_u76f8_u5173_u7684_u6280_u5de7:id4}}\begin{itemize}
\item {} 
\sphinxstylestrong{过拟合}

\end{itemize}

\begin{DUlineblock}{0em}
\item[] 发生过拟合的原因主要有两个：
\item[] 1. 模型拥有大量参数，表现力强
\item[] 2. 训练数据少
\item[] 
\end{DUlineblock}
\begin{itemize}
\item {} 
\sphinxstylestrong{权值衰减}

\end{itemize}
\begin{quote}

该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。用符号表示的话，如果将权重记为
\(\pmb{W}\)
L2范数的权值衰减就是
\(\frac{1}{2}\lambda\pmb{W}^{2}\)
，然后将这个
\(\frac{1}{2}\lambda\pmb{W}^{2}\)
加到损失函数上。这里
\(\lambda\)
是控制正则化强度的超参数。设置的越大，对大的权重施加的惩罚就越重。
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{Dropout}

\end{itemize}
\begin{quote}

如果网络的模型变得很复杂，只用权重衰减就难以应付了。在这种情况下，经常会使用Dropout方法。

Dropout是一种在学习过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。对于各神经元的输出，要乘上训练时的删除比例后再输出。

\noindent{\hspace*{\fill}\sphinxincludegraphics{{dropout}.png}\hspace*{\fill}}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{Dropout}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dropout\PYGZus{}ratio}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dropout\PYGZus{}ratio} \PYG{o}{=} \PYG{n}{dropout\PYGZus{}ratio}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mask} \PYG{o}{=} \PYG{n+nb+bp}{None}

        \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{train\PYGZus{}flg}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}\PYG{p}{:}
                \PYG{k}{if} \PYG{n}{train\PYGZus{}flg}\PYG{p}{:}
                        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mask} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{o}{*}\PYG{n}{x}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dropout\PYGZus{}ratio}
                        \PYG{k}{return} \PYG{n}{x} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mask}
                \PYG{k}{else}\PYG{p}{:}
                        \PYG{k}{return} \PYG{n}{x} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mf}{1.0} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dropout\PYGZus{}ratio}\PYG{p}{)}

        \PYG{k}{def} \PYG{n+nf}{backward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dout}\PYG{p}{)}\PYG{p}{:}
                \PYG{k}{return} \PYG{n}{dout} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mask}
\end{sphinxVerbatim}
\end{quote}


\section{超参数的验证}
\label{\detokenize{_u4e0e_u5b66_u4e60_u76f8_u5173_u7684_u6280_u5de7:id5}}\begin{itemize}
\item {} 
\sphinxstylestrong{验证数据}

\end{itemize}
\begin{quote}

调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据。我们使用这个验证数据来评估超参数的好坏。
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{超参数的最优化}

\end{itemize}
\begin{enumerate}
\item {} 
设定超参数的范围

\item {} 
从设定的超参数范围中随机采样

\item {} 
使用上一步中采样刀的超参数进行学习，通过验证数据评估识别精度（但是要将epoch设置的很小）

\item {} 
重复上述步骤，根据他们的识别精度的结果，缩小超参数的范围

\end{enumerate}


\chapter{卷积神经网络}
\label{\detokenize{_u5377_u79ef_u795e_u7ecf_u7f51_u7edc:id1}}\label{\detokenize{_u5377_u79ef_u795e_u7ecf_u7f51_u7edc::doc}}

\section{整体结构}
\label{\detokenize{_u5377_u79ef_u795e_u7ecf_u7f51_u7edc:id2}}\begin{quote}

全连接神经网络中，Affine曾后面跟着激活函数ReLU层（或Sigmoid层）

\noindent{\hspace*{\fill}\sphinxincludegraphics{{full}.png}\hspace*{\fill}}

CNN中新增了Convolution层和Pooling层

\noindent{\hspace*{\fill}\sphinxincludegraphics{{CNN}.png}\hspace*{\fill}}
\end{quote}


\section{卷积层}
\label{\detokenize{_u5377_u79ef_u795e_u7ecf_u7f51_u7edc:id3}}\begin{itemize}
\item {} 
\sphinxstylestrong{全连接层存在的问题}

\end{itemize}
\begin{quote}

全连接层存在什么问题呢？那就是数据的形状被“忽视”了。比如，输入数据是图像时，图像通常是高，长，通道方向上的3维形状。但是全连接层输入时，需要将3维数据拉平为1维数据。
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{卷积运算}

\end{itemize}
\begin{quote}

卷积运算相当于图像处理中的“滤波器运算”。

\noindent{\hspace*{\fill}\sphinxincludegraphics{{conv}.png}\hspace*{\fill}}

\noindent{\hspace*{\fill}\sphinxincludegraphics{{conv_bias}.png}\hspace*{\fill}}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{填充}

\end{itemize}
\begin{quote}

在卷积运算之前，有时要向输入数据的周围填入固定的数据（比如0等），这称为填充。使用填充主要时为了调整输出的大小。因为每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能会变为1，导致无法再应用卷积运算。

\noindent{\hspace*{\fill}\sphinxincludegraphics{{padding}.png}\hspace*{\fill}}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{步幅}

\end{itemize}
\begin{quote}

应用滤波器的位置间隔成为步幅(stride)。

\noindent{\hspace*{\fill}\sphinxincludegraphics{{stride}.png}\hspace*{\fill}}

假设输入大小为
\((H,W)\)
，滤波器的大小为
\((FH,FW)\)
，输出大小为
\((OH,OW)\)
, 填充为
\(P\)
，步幅为
\(S\)
，此时输出大小可以通过下式计算：
\begin{equation*}
\begin{split}OH = \frac{H + 2P -FH}{S} + 1\\
OW = \frac{H + 2P -FH}{S} + 1\end{split}
\end{equation*}\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{三维数据的卷积运算}

\end{itemize}
\begin{quote}

\noindent{\hspace*{\fill}\sphinxincludegraphics{{conv_3}.png}\hspace*{\fill}}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}

将数据和滤波器结合长方体来考虑，3维数据的卷积运算会很容易理解。比如，通道数为C，高度为H，长度为W的数据形状可以写成(C, H, W)。

\noindent{\hspace*{\fill}\sphinxincludegraphics{{block_conv}.png}\hspace*{\fill}}
\end{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}
\begin{quote}

如果需要在通道方向上也拥有多个卷积运算的输出，就需要用到多个滤波器（权重）。用图表示如下：

\noindent{\hspace*{\fill}\sphinxincludegraphics{{muti_conv}.png}\hspace*{\fill}}
\end{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}
\begin{quote}

加偏置

\noindent{\hspace*{\fill}\sphinxincludegraphics{{muti_bias}.png}\hspace*{\fill}}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{批处理}

\end{itemize}
\begin{quote}

\noindent{\hspace*{\fill}\sphinxincludegraphics{{batch_conv}.png}\hspace*{\fill}}
\end{quote}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}


\section{池化层}
\label{\detokenize{_u5377_u79ef_u795e_u7ecf_u7f51_u7edc:id4}}\begin{quote}

池化是缩小高，长方向上的空间运算。下图为Max池化的处理顺序。

\noindent{\hspace*{\fill}\sphinxincludegraphics{{pool}.png}\hspace*{\fill}}

除了Max池化外还有Average池化。Max是从目标区域中取出最大值，Average是计算目标区域中的平均值。
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{池化层的特征}

\end{itemize}
\begin{enumerate}
\item {} 
没有要学习的参数

\item {} 
通道数不发生变化

\item {} 
对微小的位置变化具有鲁棒性

\end{enumerate}


\section{卷积层和池化层的实现}
\label{\detokenize{_u5377_u79ef_u795e_u7ecf_u7f51_u7edc:id5}}\begin{itemize}
\item {} 
\sphinxstylestrong{基于im2col的展开}

\end{itemize}
\begin{quote}

im2col这个名称是“image to column”的缩写，对于输入数据，im2col将应用滤波器的区域横向展开为1列。

\noindent{\hspace*{\fill}\sphinxincludegraphics{{im2col}.png}\hspace*{\fill}}

卷积运算的滤波器处理细节：将滤波器纵向展开为1列，并计算和im2col展开的数据的矩阵乘积，最后转换(reshape)为输出数据的大小。
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{卷积层的实现}

\end{itemize}
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{Convolution}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{W}\PYG{p}{,} \PYG{n}{b}\PYG{p}{,} \PYG{n}{stride}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{pad}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{W} \PYG{o}{=} \PYG{n}{W}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{b} \PYG{o}{=} \PYG{n}{b}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{stride} \PYG{o}{=} \PYG{n}{stride}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pad} \PYG{o}{=} \PYG{n}{pad}

        \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{FN}\PYG{p}{,} \PYG{n}{C}\PYG{p}{,} \PYG{n}{FH}\PYG{p}{,} \PYG{n}{FW} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{W}\PYG{o}{.}\PYG{n}{shape}
                \PYG{n}{N}\PYG{p}{,} \PYG{n}{C}\PYG{p}{,} \PYG{n}{H}\PYG{p}{,} \PYG{n}{W} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{shape}
                \PYG{n}{out\PYGZus{}h} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{+} \PYG{p}{(}\PYG{n}{H} \PYG{o}{+} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pad} \PYG{o}{\PYGZhy{}} \PYG{n}{FH}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{stride}\PYG{p}{)}
                \PYG{n}{out\PYGZus{}w} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{+} \PYG{p}{(}\PYG{n}{W} \PYG{o}{+} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pad} \PYG{o}{\PYGZhy{}} \PYG{n}{FW}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{stride}\PYG{p}{)}

                \PYG{n}{col} \PYG{o}{=} \PYG{n}{im2col}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{FH}\PYG{p}{,} \PYG{n}{FW}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{stride}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pad}\PYG{p}{)}
                \PYG{n}{col\PYGZus{}w} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{W}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{FN}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T} \PYG{c+c1}{\PYGZsh{} 滤波器的展开}
                \PYG{n}{out} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{col}\PYG{p}{,} \PYG{n}{col\PYGZus{}w}\PYG{p}{)} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{b}

                \PYG{n}{out} \PYG{o}{=} \PYG{n}{out}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{N}\PYG{p}{,} \PYG{n}{out\PYGZus{}h}\PYG{p}{,} \PYG{n}{out\PYGZus{}w}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}

                \PYG{k}{return} \PYG{n}{out}
\end{sphinxVerbatim}
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{池化层的实现}

\end{itemize}
\begin{quote}

池化层的实现和卷积层相同，都使用im2col展开数据。不过，池化的情况下，在通道方向上是独立的，这点和卷积层不同。

\noindent{\hspace*{\fill}\sphinxincludegraphics{{pool_realize}.png}\hspace*{\fill}}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{Pooling}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{pool\PYGZus{}h}\PYG{p}{,} \PYG{n}{pool\PYGZus{}w}\PYG{p}{,} \PYG{n}{stride}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{pad}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pool\PYGZus{}h} \PYG{o}{=} \PYG{n}{pool\PYGZus{}h}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pool\PYGZus{}w} \PYG{o}{=} \PYG{n}{pool\PYGZus{}w}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{stride} \PYG{o}{=} \PYG{n}{stride}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pad} \PYG{o}{=} \PYG{n}{pad}

        \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{N}\PYG{p}{,} \PYG{n}{C}\PYG{p}{,} \PYG{n}{H}\PYG{p}{,} \PYG{n}{W} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{shape}
                \PYG{n}{out\PYGZus{}h} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{+} \PYG{p}{(}\PYG{n}{H} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pool\PYGZus{}h}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{stride}\PYG{p}{)}
                \PYG{n}{out\PYGZus{}w} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{+} \PYG{p}{(}\PYG{n}{W} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pool\PYGZus{}w}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{stride}\PYG{p}{)}

                \PYG{c+c1}{\PYGZsh{} 展开}
                \PYG{n}{col} \PYG{o}{=} \PYG{n}{im2col}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pool\PYGZus{}h}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pool\PYGZus{}w}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{stride}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pad}\PYG{p}{)}
                \PYG{n}{col} \PYG{o}{=} \PYG{n}{col}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pool\PYGZus{}h}\PYG{o}{*}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pool\PYGZus{}w}\PYG{p}{)}

                \PYG{c+c1}{\PYGZsh{} 最大值}
                \PYG{n}{out} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{col}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
                \PYG{c+c1}{\PYGZsh{} 转换}
                \PYG{n}{out} \PYG{o}{=} \PYG{n}{out}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{N}\PYG{p}{,} \PYG{n}{out\PYGZus{}h}\PYG{p}{,} \PYG{n}{out\PYGZus{}w}\PYG{p}{,} \PYG{n}{C}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}

                \PYG{k}{return} \PYG{n}{out}
\end{sphinxVerbatim}
\end{quote}


\section{CNN的实现}
\label{\detokenize{_u5377_u79ef_u795e_u7ecf_u7f51_u7edc:cnn}}\begin{quote}

\noindent{\hspace*{\fill}\sphinxincludegraphics{{simple_cnn}.png}\hspace*{\fill}}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{SimpleConvNet}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{input\PYGZus{}dim}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{p}{)}\PYG{p}{,}
                        \PYG{n}{conv\PYGZus{}param}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filter\PYGZus{}num}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filter\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,}
                                    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pad}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{stride}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                        \PYG{n}{hidden\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{output\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{weight\PYGZus{}init\PYGZus{}std}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{filter\PYGZus{}num} \PYG{o}{=} \PYG{n}{conv\PYGZus{}param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filter\PYGZus{}num}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
                \PYG{n}{filter\PYGZus{}size} \PYG{o}{=} \PYG{n}{conv\PYGZus{}param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filter\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
                \PYG{n}{filter\PYGZus{}pad} \PYG{o}{=} \PYG{n}{conv\PYGZus{}param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pad}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
                \PYG{n}{filter\PYGZus{}stride} \PYG{o}{=} \PYG{n}{conv\PYGZus{}param}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{stride}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
                \PYG{n}{input\PYGZus{}size} \PYG{o}{=} \PYG{n}{input\PYGZus{}dim}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
                \PYG{n}{conv\PYGZus{}output\PYGZus{}size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{input\PYGZus{}size} \PYG{o}{\PYGZhy{}} \PYG{n}{filter\PYGZus{}size} \PYG{o}{+} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{filter\PYGZus{}pad}\PYG{p}{)} \PYG{o}{/} \PYGZbs{}
                                    \PYG{n}{filter\PYGZus{}stride} \PYG{o}{+}\PYG{l+m+mi}{1}
                \PYG{n}{pool\PYGZus{}output\PYGZus{}size} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{filter\PYGZus{}num} \PYG{o}{*} \PYG{p}{(}\PYG{n}{conv\PYGZus{}output\PYGZus{}size}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{*}
                                   \PYG{p}{(}\PYG{n}{conv\PYGZus{}output\PYGZus{}size}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}

                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{weight\PYGZus{}init\PYGZus{}std} \PYG{o}{*} \PYGZbs{}
                                    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{filter\PYGZus{}num}\PYG{p}{,} \PYG{n}{input\PYGZus{}dim}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                                                        \PYG{n}{filter\PYGZus{}size}\PYG{p}{,} \PYG{n}{filter\PYGZus{}size}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{filter\PYGZus{}num}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{weight\PYGZus{}init\PYGZus{}std} \PYG{o}{*} \PYGZbs{}
                                    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{pool\PYGZus{}output\PYGZus{}size}\PYG{p}{,}
                                                        \PYG{n}{hidden\PYGZus{}size}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}size}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{weight\PYGZus{}init\PYGZus{}std} \PYG{o}{*} \PYGZbs{}
                                    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}size}\PYG{p}{,} \PYG{n}{output\PYGZus{}size}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{output\PYGZus{}size}\PYG{p}{)}

                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers} \PYG{o}{=} \PYG{n}{OrderedDict}\PYG{p}{(}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Conv1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{Convolution}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                                   \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                                   \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{stride}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                                   \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pad}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Relu1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{Relu}\PYG{p}{(}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pool1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{Pooling}\PYG{p}{(}\PYG{n}{pool\PYGZus{}h}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{pool\PYGZus{}w}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{stride}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{Affine}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Relu2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{Relu}\PYG{p}{(}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{Affine}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{last\PYGZus{}layer} \PYG{o}{=} \PYG{n}{softmaxwithloss}\PYG{p}{(}\PYG{p}{)}

        \PYG{k}{def} \PYG{n+nf}{predict}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
                \PYG{k}{for} \PYG{n}{layer} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                        \PYG{n}{x} \PYG{o}{=} \PYG{n}{layer}\PYG{o}{.}\PYG{n}{forward}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
                \PYG{k}{return} \PYG{n}{x}

        \PYG{k}{def} \PYG{n+nf}{loss}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{y} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
                \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lastLayer}\PYG{o}{.}\PYG{n}{forward}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}

        \PYG{k}{def} \PYG{n+nf}{gradient}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}\PYG{p}{:}
                \PYG{c+c1}{\PYGZsh{} forward}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{loss}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}

                \PYG{c+c1}{\PYGZsh{} backward}
                \PYG{n}{dout} \PYG{o}{=} \PYG{l+m+mi}{1}
                \PYG{n}{dout} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lastLayer}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{n}{dout}\PYG{p}{)}

                \PYG{n}{layers} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
                \PYG{n}{layers}\PYG{o}{.}\PYG{n}{reverse}\PYG{p}{(}\PYG{p}{)}
                \PYG{k}{for} \PYG{n}{layer} \PYG{o+ow}{in} \PYG{n}{layers}\PYG{p}{:}
                        \PYG{n}{dout} \PYG{o}{=} \PYG{n}{layer}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{n}{dout}\PYG{p}{)}

                \PYG{c+c1}{\PYGZsh{} 设定}
                \PYG{n}{grads} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
                \PYG{n}{grads}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Conv1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dW}
                \PYG{n}{grads}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Conv1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{db}
                \PYG{n}{grads}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dW}
                \PYG{n}{grads}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{db}
                \PYG{n}{grads}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dW}
                \PYG{n}{grads}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Affine2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{db}
\end{sphinxVerbatim}
\end{quote}


\section{具有代表性的CNN}
\label{\detokenize{_u5377_u79ef_u795e_u7ecf_u7f51_u7edc:id6}}\begin{itemize}
\item {} 
\sphinxstylestrong{LeNet}

\end{itemize}
\begin{quote}

\noindent{\hspace*{\fill}\sphinxincludegraphics{{lenet}.png}\hspace*{\fill}}

和“现在的CNN”相比，LeNet中使用sigmoid函数，而现在的CNN中主要使用ReLU函数。此外，原始的LeNet中使用子采样(subsampling)缩小中间数据的大小，而现在的CNN中Max池化是主流。
\end{quote}
\begin{itemize}
\item {} 
\sphinxstylestrong{AlexNet}

\end{itemize}
\begin{quote}

\noindent{\hspace*{\fill}\sphinxincludegraphics{{alexnet}.png}\hspace*{\fill}}

AlexNet叠有多个卷积层和池化层，最后经由全连接层输出结果。虽然结构上AlexNet和LeNet没有大的不同，但有以下几点差异：
\begin{enumerate}
\item {} 
激活函数使用ReLU

\item {} 
使用进行局部正规化的LRN(Local Response Normalization)层

\item {} 
使用Rropout

\end{enumerate}
\end{quote}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{索引}
\printindex
\end{document}