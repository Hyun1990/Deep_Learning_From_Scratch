

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>5. 与学习相关的技巧 &mdash; Deep_Learning_From_Scratch 1.0 文档</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="索引"
              href="genindex.html"/>
        <link rel="search" title="搜索" href="search.html"/>
    <link rel="top" title="Deep_Learning_From_Scratch 1.0 文档" href="index.html"/>
        <link rel="next" title="6. 卷积神经网络" href="卷积神经网络.html"/>
        <link rel="prev" title="4. 误差反向传播法" href="误差反向传播法.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Deep_Learning_From_Scratch
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="感知机.html">1. 感知机</a></li>
<li class="toctree-l1"><a class="reference internal" href="神经网络.html">2. 神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="神经网络的学习.html">3. 神经网络的学习</a></li>
<li class="toctree-l1"><a class="reference internal" href="误差反向传播法.html">4. 误差反向传播法</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">5. 与学习相关的技巧</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">5.1. 参数的更新</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">5.2. 权重的初始值</a></li>
<li class="toctree-l2"><a class="reference internal" href="#batch-normalization">5.3. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">5.4. 正则化</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">5.5. 超参数的验证</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="卷积神经网络.html">6. 卷积神经网络</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Deep_Learning_From_Scratch</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>5. 与学习相关的技巧</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/与学习相关的技巧.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1>5. 与学习相关的技巧<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<div class="section" id="id2">
<h2>5.1. 参数的更新<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<p>神经网络学习的目的是找到使损失函数的值尽可能小的参数。</p>
<ul>
<li><p class="first"><strong>SGD</strong></p>
<div class="math">
\[\pmb{W} \leftarrow \pmb{W} - \eta\frac{\partial{L}}{\partial\pmb{W}}\]</div>
</li>
</ul>
<blockquote>
<div><p>将SGD实现为一个Python的类</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SGD</span><span class="p">:</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                        <span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
<p>使用SGD，可以按如下方式进行神经网络的参数更新</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span> <span class="o">=</span> <span class="n">get_mini_batch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="c1"># mini_batch</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">params</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
        <span class="o">...</span>
</pre></div>
</div>
</div></blockquote>
<p><em>SGD的缺点</em></p>
<div class="line-block">
<div class="line">如果函数的形状非均向，搜索的路径就会非常低效</div>
<div class="line">SGD低效的根本原因是梯度并没有指向最小值的方向</div>
</div>
</div></blockquote>
<ul class="simple">
<li><strong>Momentum</strong></li>
</ul>
<blockquote>
<div><p>Momentum是在指数加权平均的基础上提出的</p>
<p><strong>指数加权平均可用如下公式表示：</strong></p>
<blockquote>
<div><div class="math">
\[v_t=k * v_{t-1} + (1-k)w_t\]</div>
</div></blockquote>
<div class="line-block">
<div class="line">核心：使用前多少个值来表示当前的趋势值Vt</div>
<div class="line">k的值决定了趋势值v应受到前多少个值的影响，w代表当前值</div>
<div class="line">参考文档 <a class="reference external" href="http://captainbed.top/2-2-3/">http://captainbed.top/2-2-3/</a></div>
<div class="line">用一行代码表示指数加权平均：</div>
</div>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">v</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="mi">100</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">kv</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">k</span><span class="p">)</span><span class="n">w_t</span>
</pre></div>
</div>
</div></blockquote>
<p>Momentum的公式表示如下：</p>
<blockquote>
<div><div class="math">
\[\begin{split}v \leftarrow \alpha{v} - \eta\frac{\partial{L}}{\partial\pmb{W}}\\
\pmb{W} \leftarrow \pmb{w} + v\end{split}\]</div>
</div></blockquote>
<p>将Momentum实现为一个Python类</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Momentum</span><span class="p">:</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="p">{}</span>
                        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="o">*</span><span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                        <span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
</div></blockquote>
<ul class="simple">
<li><strong>AdaGrad</strong></li>
</ul>
<blockquote>
<div><p>AdaGrad会为参数的每个元素适当地调整学习率</p>
<p>AdaGrad的公式如下：</p>
<blockquote>
<div><div class="math">
\[\begin{split}\pmb{h} \leftarrow \pmb{h} + \frac{\partial{L}}{\partial\pmb{W}} \odot \frac{\partial{L}}{\partial\pmb{W}}\\
\pmb{W} \leftarrow \pmb{W} - \eta\frac{1}{\sqrt{\pmb{h}}}\frac{\partial{L}}{\partial\pmb{W}}\end{split}\]</div>
</div></blockquote>
<p>这里出现了新变量h，它保存了以前的所有梯度的平方和，在更新参数时，通过乘以其根号倒数，就可以调整学习的尺度。这意味着参数中的元素中变动较大（被大幅更新的元素的学习率会将变小）。</p>
<p>将AdaGrad实现为一个Python类</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AdaGrad</span><span class="p">:</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                <span class="n">parsms</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</div></blockquote>
<ul class="simple">
<li><strong>RMSprop</strong></li>
</ul>
<blockquote>
<div><p>AdaGrad会记录过去所有的梯度平方和，而RMSprop则逐渐遗忘过去的梯度，在做加法时将新梯度的信息更多的反映出来。</p>
<ol class="arabic simple">
<li>首先计算出dw和db</li>
<li>计算出指数平均</li>
</ol>
<blockquote>
<div><div class="math">
\[\begin{split}s_{dw} = ks_{dw} + (1-k)dw^{2}\\
s_{db} = ks_{db} + (1-k)db^{2}\end{split}\]</div>
</div></blockquote>
<ol class="arabic simple" start="3">
<li>更新w和b</li>
</ol>
<blockquote>
<div><div class="math">
\[\begin{split}w = w - r(dw/sqrt(s_{dw}))\\
b = b - r(db/sqrt(s_{db}))\end{split}\]</div>
</div></blockquote>
</div></blockquote>
<ul class="simple">
<li><strong>Adam</strong></li>
</ul>
<blockquote>
<div><p>Adam融合了Momentum和AdaGrad方法</p>
<ol class="arabic simple">
<li>算出dw，db</li>
<li>求出动量指数平均</li>
</ol>
<blockquote>
<div><div class="math">
\[\begin{split}v_{dw} = k_1v_{dw}+(1-k_1)dw\\
v_{db} = k_1v_{db}+(1-k_1)db\end{split}\]</div>
</div></blockquote>
<ol class="arabic simple" start="3">
<li>求出RMPprop指数平均</li>
</ol>
<blockquote>
<div><div class="math">
\[\begin{split}s_{dw} = k_2s_{dw}+(1-k_2)dw^{2}\\
s_{db} = k_2s_{db}+(1-k_2)db^{2}\end{split}\]</div>
</div></blockquote>
<ol class="arabic simple" start="4">
<li>对指数平均进行修正</li>
</ol>
<blockquote>
<div><div class="math">
\[\begin{split}v^{c}dw = v_{dw}/(1-k_1^{t})\\
v^{c}db = v_{db}/(1-k_1^{t})\\
s^{c}dw = s_{dw}/(1-k_2^{t})\\
    s^{c}db = s_{db}/(1-k_2^{t})\end{split}\]</div>
</div></blockquote>
<ol class="arabic simple" start="5">
<li>通过两个指数平均更新参数</li>
</ol>
<blockquote>
<div><div class="math">
\[\begin{split}w = w - r(v^{c}_{dw}/sqrt(s^{c}_{dw}+u))\\
b = b - r(v^{c}_{db}/sqrt(s^{c}_{db}+u))\end{split}\]</div>
</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="id3">
<h2>5.2. 权重的初始值<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><strong>权重初始值能设置成0吗？</strong></li>
</ul>
<blockquote>
<div>将权重初始值设置成一样的值，在误差反向传播法中，所有的权重值会进行相同的更新，拥有了对称的值（重复的值）。为防止“权重均一化”，必须随机生成初始值。</div></blockquote>
<ul class="simple">
<li><strong>隐藏层激活值的分布</strong></li>
</ul>
<blockquote>
<div>各层激活值的分布要求要适当的广度，通过在各层之间传递多样性的数据，神经网络就可以进行高效的学习。如果传递的是有所偏向的数据，就会出现梯度消失或“表现力受限”的问题，导致学习可能无法顺利进行。</div></blockquote>
<ul class="simple">
<li><strong>Xavier初始值</strong></li>
</ul>
<blockquote>
<div><p>如果前一层的节点数为
<span class="math">\(n\)</span>
，则初始值使用标准差为
<span class="math">\(\frac{1}{\sqrt{n}}\)</span>
的分布</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">node_num</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># 前一层的节点数</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">node_num</span><span class="p">,</span> <span class="n">node_num</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">node_num</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<ul class="simple">
<li><strong>ReLU的权重初始值He</strong></li>
</ul>
<blockquote>
<div><p>当前一层的节点数为
<span class="math">\(n\)</span>
，则初始值使用标准差为
<span class="math">\(\sqrt{\frac{2}{n}}\)</span>
的分布</p>
<p>激活函数使用ReLU时，不同权重初始值的激活值分布的变化如下图</p>
<img alt="_images/activations.png" class="align-center" src="_images/activations.png" />
<p><strong>当激活函数使用ReLU时，权重初始值使用He初始值。当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值。</strong></p>
</div></blockquote>
</div>
<div class="section" id="batch-normalization">
<h2>5.3. Batch Normalization<a class="headerlink" href="#batch-normalization" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>Batch Norm的思路是调整各层激活值分布使其拥有适当的广度。为此，要向神经网络中插入对数据分布进行正规化的层，即Batch Normalization曾。</p>
<img alt="_images/batch.png" class="align-center" src="_images/batch.png" />
<p>Batch Norm以进行学习时的nimi-batch为单位，按mini-batch进行正规化。具体而言就是使数据分布的均值为0，方差为1的正规化。如下式表示：</p>
<div class="math">
\[\begin{split}\begin{align}
    \mu_B \leftarrow \frac{1}{m}\sum_{i=1}^{m}x_i\\
\sigma^{2}_B \leftarrow \frac{1}{m}\sum_{i=1}^m{(x_i - \mu_B)}^{2}\\
    \hat{x}_i \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma^{2}_B + \epsilon}}
    \end{align}\end{split}\]</div>
<p>nimi-batch将输入的数据
<span class="math">\(\{x_1,x_2,...,x_m\}\)</span>
变换为均值为0，方差为1的数据
<span class="math">\(\{\hat{x}_1,\hat{x}_2,...,\hat{x}_m\}\)</span>
。通过将这个处理插入刀激活函数的前面（或者后面），可以减小数据分布的偏向。接着，Batch Norm会对正规化后的数据进行缩放和平移的变换：</p>
<div class="math">
\[y_i \leftarrow \gamma\hat{x}_i + \beta\]</div>
<p>这里，
<span class="math">\(\gamma\)</span>
和
<span class="math">\(\beta\)</span>
是参数。一开始
<span class="math">\(\gamma = 1, \beta = 0\)</span>
，然后再通过学习调整到合适的值。</p>
</div></blockquote>
<div class="line-block">
<div class="line"><strong>Batch Norm的优点：</strong></div>
<div class="line">1. 可以使学习快速进行（可以增大学习率）</div>
<div class="line">2. 不那么依赖初始值（对于初始值不那么神经质）</div>
<div class="line">3. 抑制过拟合（降低Dropout等的必要性）</div>
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="id4">
<h2>5.4. 正则化<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><strong>过拟合</strong></li>
</ul>
<div class="line-block">
<div class="line">发生过拟合的原因主要有两个：</div>
<div class="line">1. 模型拥有大量参数，表现力强</div>
<div class="line">2. 训练数据少</div>
<div class="line"><br /></div>
</div>
<ul class="simple">
<li><strong>权值衰减</strong></li>
</ul>
<blockquote>
<div>该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。用符号表示的话，如果将权重记为
<span class="math">\(\pmb{W}\)</span>
L2范数的权值衰减就是
<span class="math">\(\frac{1}{2}\lambda\pmb{W}^{2}\)</span>
，然后将这个
<span class="math">\(\frac{1}{2}\lambda\pmb{W}^{2}\)</span>
加到损失函数上。这里
<span class="math">\(\lambda\)</span>
是控制正则化强度的超参数。设置的越大，对大的权重施加的惩罚就越重。</div></blockquote>
<ul class="simple">
<li><strong>Dropout</strong></li>
</ul>
<blockquote>
<div><p>如果网络的模型变得很复杂，只用权重衰减就难以应付了。在这种情况下，经常会使用Dropout方法。</p>
<p>Dropout是一种在学习过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。对于各神经元的输出，要乘上训练时的删除比例后再输出。</p>
<img alt="_images/dropout.png" class="align-center" src="_images/dropout.png" />
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Dropout</span><span class="p">:</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dropout_ratio</span> <span class="o">=</span> <span class="n">dropout_ratio</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train_flg</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">train_flg</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_ratio</span>
                        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span>
                <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_ratio</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="id5">
<h2>5.5. 超参数的验证<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><strong>验证数据</strong></li>
</ul>
<blockquote>
<div>调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据。我们使用这个验证数据来评估超参数的好坏。</div></blockquote>
<ul class="simple">
<li><strong>超参数的最优化</strong></li>
</ul>
<blockquote>
<div><ol class="arabic simple">
<li>设定超参数的范围</li>
<li>从设定的超参数范围中随机采样</li>
<li>使用上一步中采样刀的超参数进行学习，通过验证数据评估识别精度（但是要将epoch设置的很小）</li>
<li>重复上述步骤，根据他们的识别精度的结果，缩小超参数的范围</li>
</ol>
</div></blockquote>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="卷积神经网络.html" class="btn btn-neutral float-right" title="6. 卷积神经网络" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="误差反向传播法.html" class="btn btn-neutral" title="4. 误差反向传播法" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, hyun.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>